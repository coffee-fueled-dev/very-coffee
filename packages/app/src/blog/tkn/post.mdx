## Overview

I've been working on the tkn project for a few years as of writing this. It started in the basement of my friend's house watching a [Two Minute Papers](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg) video on Youtube about GPT3 before it was officially released.

That video made me think a lot about how computers can learn about the world. The first, and most intuitive analogy I could think of was a child learning to read. I imagined how the letters look foreign at first, and then gradually the child learns words by associating frequently paired letters with each other. Eventually, a word just "looks" like a cohesive entity rather than an ensemble of letters.

From that thought, I thought of the core loop of the tkn algorithm using the following constraints:

- It must ingest one character at a time
- It must learn the concept of a word be character association without first being told anything about language
- It must be able to reproduce the original text

In psuedo code:
```
initialize:
  dictionary ← empty (tracks all seen patterns)
  buffer ← empty (accumulates current sequence)
  pattern ← "" (string representation of buffer)

for each input:
  extended ← pattern + input

  if dictionary contains extended:
    // Inclusion heuristic: grow the pattern
    pattern ← extended
    add input to buffer
  else:
    // Pattern boundary detected: emit and reset
    if buffer is not empty:
      emit buffer as discovered sequence

    dictionary learns extended
    buffer ← [input]
    pattern ← input
```
