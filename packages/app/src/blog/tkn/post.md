## Overview

I've been working on the tkn project for a few years as of writing this. It started in the basement of my friend's house watching a [Two Minute Papers](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg) video on Youtube in (I think) 2022 about GPT3.

That video made me think a lot about how computers can learn to reason about the world. The first, and most intuitive analogy I could think of was imagining a child learning to read. I imagined how the letters look foreign at first, and then gradually the child learns words by more seamlessly combining the letters they associate with each other. Eventually, a word just "looks" like a cohesive entity rather than an ensemble of characters.

From that thought, I imagined an algorithm that would imitate a child learning to read and thought of the core loop of the tkn algorithm.
The constraints I put on the problem were:

- It must ingest one character at a time
- It must learn the concept of a word without first being told anything about language
- It must be able to reproduce the original text
